{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paper Reproduction: \"ALBERTA’S FISCAL RESPONSES TO FLUCTUATIONS IN NON-RENEWABLE-RESOURCE REVENUE\" in python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This is my replication of the empirical results, tables, and figures produced in a paper by Dr. Ergete Ferede, published by the University of Calgary school of public policy in Volume 11:24, September 2018.\n",
    "\n",
    "The original paper is here: [https://www.policyschool.ca/wp-content/uploads/2018/09/NRR-Ferede.pdf](https://www.policyschool.ca/wp-content/uploads/2018/09/NRR-Ferede.pdf)\n",
    "\n",
    "I chose this paper to reproduce for two reasons. The first is pragmatic; the data it uses is all publicly available, so I actually can. The second is that it describes a topic of importance in the province of Alberta, where I live.\n",
    "\n",
    "You can read the details of what the paper sets out to show in the paper itself, but in brief the idea is to show that provincial government spending increases in the year following an increase in non-renewable resource revenue, but it does not decrease accordingly in the year following declines in the same revenue source. This has a ratcheting effect on public finance that is a contributor to the \"royalty rollercoaster\" that is Alberta's public finance.\n",
    "\n",
    "In the following sections I'll go through the code necessary to extract and transform the data set used in the paper, as well as reproduce its key empirical results. Since most economists don't use python, and they make up a key part of my intended audience for this, I'll be adding comments to my code that explicitly describe what some of the functions and methods I'm calling do.\n",
    "\n",
    "I'm including all of the code necessary to produce this reproduction, since that's a big part of why I'm doing this exercise, but if you're just interested in seeing how my reproduced results compare to the original paper you can skip all the code blocks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and data acquisition\n",
    "\n",
    "This section of the code loads required modules, downloads the required data sets, and reads them into DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import datetime as dt\n",
    "import requests\n",
    "import pandas as pd\n",
    "import pandas_datareader as pdr\n",
    "import numpy as np\n",
    "import stats_can\n",
    "import altair as alt\n",
    "import seaborn as sns\n",
    "from arch.unitroot import DFGLS, ADF, PhillipsPerron\n",
    "from statsmodels.tsa.api import VAR\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by loading the required libraries that will be used to support the analysis. For reference here are links to the libraries that are being used:\n",
    "\n",
    "* [Pathlib](https://docs.python.org/3/library/pathlib.html)\n",
    "* [datetime](https://docs.python.org/3/library/datetime.html)\n",
    "* [requests](https://requests.readthedocs.io/en/master/)\n",
    "* [pandas](https://pandas.pydata.org/)\n",
    "* [pandas_datareader](https://pandas-datareader.readthedocs.io/en/latest/)\n",
    "* [numpy](https://numpy.org/)\n",
    "* [stats_can](https://stats-can.readthedocs.io/en/latest/)\n",
    "* [altair](https://altair-viz.github.io/)\n",
    "* [seaborn](https://seaborn.pydata.org/)\n",
    "* [arch](https://arch.readthedocs.io/en/latest/index.html)\n",
    "* [statsmodels](https://www.statsmodels.org/stable/index.html)\n",
    "* [matplotlib](https://matplotlib.org/stable/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Historical budget data\n",
    "\n",
    "Functions in this section are concerned with acquiring historical Alberta budget data and reading it into a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_budget_data() -> Path:\n",
    "    \"\"\"Download the excel file for the analysis from the policy school page.\n",
    "\n",
    "    Note the readme sheet on the first file. Credit to Kneebone and Wilkins for\n",
    "    assembling it, and policy school for hosting it.\n",
    "    \n",
    "    Originally used this URL, but found it was missing some later heritage\n",
    "    contributions. After discussion with Dr. Kneebone an updated set has been provided\n",
    "    https://www.policyschool.ca/wp-content/uploads/2019/01/Provincial-Government-Budget-Data-January-2019FINAL-USE.xlsx\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pathlib.Path\n",
    "        A path object with the location and name of the data\n",
    "    \"\"\"\n",
    "    print('Downloading data set')\n",
    "\n",
    "    url = 'https://www.policyschool.ca/wp-content/uploads/2019/03/Provincial-Government-Budget-Data-March-2019.xlsx'\n",
    "    # send a request to the url for the file\n",
    "    response = requests.get(\n",
    "        url,\n",
    "        stream=True,\n",
    "        headers={'user-agent': None}\n",
    "    )\n",
    "    # create a path object for the file in the data folder above\n",
    "    # where this notebook is saved with the file named\n",
    "    # budget.xlsx for easy later access.\n",
    "    fname = Path('.').joinpath('data').joinpath('budgets.xlsx')\n",
    "    # write the response from the request to the file in the path specified above\n",
    "    with open (fname, 'wb') as outfile:\n",
    "        for chunk in response.iter_content(chunk_size=512):\n",
    "            if chunk: # filter out keep-alive new chunks\n",
    "                outfile.write(chunk)\n",
    "    # Return the location of the file so we can load it later easily\n",
    "    return fname\n",
    "\n",
    "\n",
    "def get_budget_file(force_update: bool=False) -> Path:\n",
    "    \"\"\"Get the budget file, downloading if required.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    force_update: bool\n",
    "        Download the data file even if you already have it\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pathlib.Path\n",
    "        A path object with the location and name of the data\n",
    "    \"\"\"\n",
    "    # This is where we're expecting the file to be saved if it exists\n",
    "    fname = Path('.').joinpath('data').joinpath('budgets.xlsx')\n",
    "    if not fname.exists() or force_update:\n",
    "        download_budget_data()\n",
    "    return fname\n",
    "\n",
    "\n",
    "def get_date_index(df: pd.DataFrame) -> pd.DatetimeIndex:\n",
    "    \"\"\"Helper function to turn budget year strings into datetimes.\n",
    "\n",
    "    The Fiscal year columns span across years, e.g. 1965-66. In order\n",
    "    to use all the date indexed functionality I want to convert them into\n",
    "    an actual datetime format. This function accomplishes that\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df: pd.DataFrame\n",
    "        The budget dataframe with the fiscal year style columns\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DatetimeIndex\n",
    "        A datetime index showing January 1 of the beginning of each\n",
    "        fiscal year for each period.    \n",
    "    \"\"\"\n",
    "    date_index = pd.to_datetime(\n",
    "        df\n",
    "        .assign(year=lambda df: df['budget_yr'].str[0:4].astype(int))\n",
    "        .assign(month=1)\n",
    "        .assign(day=1)\n",
    "        [['year', 'month', 'day']]\n",
    "    )\n",
    "    return date_index\n",
    "\n",
    "\n",
    "def read_ab_budget() -> pd.DataFrame:\n",
    "    \"\"\"Read Alberta budget data.\n",
    "\n",
    "    Downloads the data if necessary, reads it in and gives\n",
    "    the variables easier to work with names\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Alberta's revenue and expenditure tables\n",
    "    \"\"\"\n",
    "    # Get the budget file, download if necessary  using functions\n",
    "    # defined above\n",
    "    fname = get_budget_file()\n",
    "    df = (\n",
    "        pd.read_excel(\n",
    "            fname,\n",
    "            sheet_name='Alberta',\n",
    "            # column titles are spaced over 3 rows\n",
    "            header=3,\n",
    "            # first column of data is B\n",
    "            index_col=1,\n",
    "            # there's a big footnote at the bottom we want to skip\n",
    "            skipfooter=21\n",
    "        )\n",
    "        # Because of the merged cells we get an empty first row\n",
    "        .loc[lambda x: x.index.notnull()]\n",
    "        # Not sure where the empty first column comes from but drop it\n",
    "        .drop(columns='Unnamed: 0')\n",
    "        .reset_index()\n",
    "        .rename(columns={\n",
    "            'index': 'budget_yr',\n",
    "            'Personal Income Tax': 'personal_income_tax',\n",
    "            'Corporation Income Tax': 'corporate_income_tax',\n",
    "            'Retail Sales Tax': 'retail_sales_tax',\n",
    "            'Federal Cash Transfers': 'federal_cash_transfers',\n",
    "            'Natural Resource Revenue': 'natural_resource_revenue',\n",
    "            'Other Own-Source Revenue': 'other_own_source_revenue',\n",
    "            'Total Revenue': 'total_revenue',\n",
    "            'Health': 'health_exp',\n",
    "            'Social Services': 'social_services_exp',\n",
    "            'Education': 'education_exp',\n",
    "            'Other Program Expenditures': 'other_program_exp',\n",
    "            'Total Program Expenditures': 'total_prog_exp',\n",
    "            'Debt Service': 'debt_service',\n",
    "            'Total  Expenditures': 'total_exp',\n",
    "            'Unnamed: 16': 'annual_deficit'\n",
    "        })\n",
    "        # Turn the fiscal year string into a datetime object\n",
    "        .assign(budget_dt=lambda df: get_date_index(df))\n",
    "        .set_index('budget_dt')\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "def read_heritage() -> pd.DataFrame:\n",
    "    \"\"\"Read deposits to the heritage trust fund from a separate table.\n",
    "\n",
    "    The paper nets out contributions to the heritage trust fund when they are\n",
    "    made, so we have to read them in to be able to net them out of resource revenue.\n",
    "\n",
    "    They're stored in the same sheet of the workbook, just down below the big table we\n",
    "    read in with the function above.\n",
    "    \"\"\"\n",
    "    fname = get_budget_file()\n",
    "    df = (\n",
    "        pd.read_excel(\n",
    "            fname,\n",
    "            sheet_name='Alberta',\n",
    "            # Have to manually specify column names because of\n",
    "            # how the table is laid out\n",
    "            header=None,\n",
    "            usecols='D:G',\n",
    "            names=['budget_yr', 'resource_allocation', 'deposits', 'advance_edu'],\n",
    "            skiprows=71,\n",
    "            skipfooter=1\n",
    "        )\n",
    "        # more fiddly cleaning because of how the table is set up\n",
    "        # there's a blank row between 1986-87 and when\n",
    "        # contributions resume in 2005-06\n",
    "        .loc[lambda df: ~df['budget_yr'].isna()]\n",
    "        .set_index('budget_yr')\n",
    "        # missing entries have 0 contributions for that\n",
    "        # category in that year\n",
    "        .fillna(0)\n",
    "        # The three columns are all counted the same\n",
    "        # for the purposes of this analysis, they just have\n",
    "        # different labels/classifications depending on the year\n",
    "        .assign(total_heritage=lambda df: df.sum(axis='columns'))\n",
    "        # Add a dummy variable to indicate heritage fund deposit years\n",
    "        .assign(heritage_dummy=1)\n",
    "        .reset_index()\n",
    "        # convert the fiscal year column to a datetime index\n",
    "        .assign(budget_dt=lambda df: get_date_index(df))\n",
    "        .drop(columns='budget_yr')\n",
    "        .set_index('budget_dt')\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "def clean_budget() -> pd.DataFrame:\n",
    "    \"\"\"Combine base budget with heritage deposits.\n",
    "    \n",
    "    Pull all the logic together to create one dataframe with all the\n",
    "    fiscal data for the period of interest.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        The full nominal budget data set.\n",
    "    \"\"\"\n",
    "    budg = read_ab_budget()\n",
    "    heritage = read_heritage()\n",
    "    budg_clean = (\n",
    "        # Start with the budget dataframe\n",
    "        budg\n",
    "        # consolidate some revenue categories\n",
    "        .assign(other_revenue=lambda df: df[['retail_sales_tax', 'federal_cash_transfers', 'other_own_source_revenue']].sum(axis='columns'))\n",
    "        # Just keep the columns we still need\n",
    "        .reindex(columns=['personal_income_tax', 'corporate_income_tax', 'natural_resource_revenue', 'other_revenue', 'total_prog_exp', 'debt_service'])\n",
    "        # add in the heritage contributions data\n",
    "        .merge(heritage[['total_heritage', 'heritage_dummy']], how='left', left_index=True, right_index=True)\n",
    "        # Set contributions and the heritage dummy to 0 for years where there were no contributions\n",
    "        .fillna(0)\n",
    "        # Net out heritage contributions from natural resources revenue\n",
    "        .assign(natural_resource_revenue_before_heritage=lambda df: df['natural_resource_revenue'])\n",
    "        .assign(natural_resource_revenue=lambda df: df['natural_resource_revenue'] - df['total_heritage'])\n",
    "        # consolidate revenue\n",
    "        .assign(total_revenue=lambda df: df[['personal_income_tax', 'corporate_income_tax', 'natural_resource_revenue', 'other_revenue']].sum(axis='columns'))\n",
    "        # consolidate expenditure\n",
    "        .assign(total_expenditure=lambda df: df[['total_prog_exp', 'debt_service']].sum(axis='columns'))\n",
    "        # calculate the deficit\n",
    "        .assign(deficit=lambda df: df['total_expenditure'] - df['total_revenue'])\n",
    "        # make all the budget numbers floating point\n",
    "        .astype('float64')\n",
    "    )\n",
    "    return budg_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real Per Capita budget\n",
    "\n",
    "All of the analysis in the paper is done in terms of real per-capita data. Functions in this section transform the nominal total budget numbers acquired in the previous section into real per-capita figures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def periodic_to_budget_annual(df: pd.DataFrame, index_name: str, year_periods: int = 4) -> pd.DataFrame:\n",
    "    \"\"\"Take a monthly or quarterly indexed dataframe and annualize it by budget period.\n",
    "\n",
    "    The inflation and population data we need to convert the budget into\n",
    "    real per-capita figures are monthly series. We need to get the average\n",
    "    population and price level for each fiscal year in the data set.\n",
    "\n",
    "    Rolling mean indexed on January year N+1 is the March to March\n",
    "    average population for fiscal year N\n",
    "    Applying a date offset of -1 year and taking only\n",
    "    January data of these rolling means gives us an average on the\n",
    "    same basis as the budget dates.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: pandas.DataFrame\n",
    "        DataFrame to be piped into this function\n",
    "    index_name: str\n",
    "        The name of the date index\n",
    "    year_periods: int, default 4\n",
    "        4 for quarterly data (population), 12 for monthly (inflation)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        An annualized dataframe on a fiscal year basis for comparison\n",
    "        to annual budget figures.\n",
    "    \"\"\"\n",
    "    df = (\n",
    "        df\n",
    "        .copy()\n",
    "        .rolling(year_periods, closed='left')\n",
    "        .mean()\n",
    "        .reset_index()\n",
    "        .assign(budget_dt=lambda df: df[index_name] - pd.DateOffset(years=1))\n",
    "        .loc[lambda x: x['budget_dt'].dt.year >= 1965]\n",
    "        .loc[lambda x: x['budget_dt'].dt.month == 1]\n",
    "        .drop(columns=index_name)\n",
    "        .set_index('budget_dt')\n",
    "        .copy()\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "def per_capita_data() -> pd.DataFrame:\n",
    "    \"\"\"Read in population data to calculate per capita estimates.\n",
    "\n",
    "    Quarterly population estimates for Alberta from Statistics Canada\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Fiscal year annualized population estimates for Alberta over the\n",
    "        reference period.\n",
    "    \"\"\"\n",
    "    table = '17-10-0009-01'\n",
    "    df = (\n",
    "        stats_can.table_to_df(table, path='data')\n",
    "        .loc[lambda x: x['GEO'] == 'Alberta']\n",
    "        .loc[lambda x: x['REF_DATE'] >= '1965']\n",
    "        .set_index('REF_DATE')\n",
    "        [['VALUE']]\n",
    "        .rename(columns={'VALUE' : 'population'})\n",
    "        .pipe(periodic_to_budget_annual, 'REF_DATE', 4)\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "def inflation_data() -> pd.DataFrame:\n",
    "    \"\"\"Read in inflation data to calculate real dollar estimates.\n",
    "\n",
    "    The whole series is scaled so 2017 budget year is = 1\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Fiscal year annualized inflation data for Alberta over\n",
    "        the reference period. Normalized to 2017 = 1\n",
    "    \"\"\"\n",
    "    # Alberta inflation doesn't go back far enough, use Canada for earlier dates\n",
    "    vecs = ('v41692327', 'v41690973')\n",
    "    df = (\n",
    "        stats_can.vectors_to_df_local(vecs, path='data', start_date=dt.date(1965, 1, 1))\n",
    "        .rename(columns={'v41692327': 'ab_inflation', 'v41690973': 'ca_inflation'})\n",
    "    )\n",
    "    # fill in with Canadian inflation data where (early) Alberta inflation data is missing.\n",
    "    mask = df['ab_inflation'].isna()\n",
    "    # Could probably do some interpolation or scaling before this, but I looked\n",
    "    # at the raw series and they were pretty comparable\n",
    "    df.loc[mask, 'ab_inflation'] = df.loc[mask, 'ca_inflation']\n",
    "    df = (\n",
    "        df\n",
    "        .drop(columns='ca_inflation')\n",
    "        .pipe(periodic_to_budget_annual, 'REF_DATE', 12)\n",
    "    )\n",
    "    # Rescale to 2017 = 100 (this is fiscal year 2017,\n",
    "    # original may have done calendar year)\n",
    "    inf_2017 = float(df.loc['2017', 'ab_inflation'])\n",
    "    df = df / inf_2017\n",
    "    return df\n",
    "\n",
    "\n",
    "def budget_real_per_capita() -> pd.DataFrame:\n",
    "    \"\"\"Get budget data in real per-capita terms.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Budget data in real per-capita terms.\n",
    "    \"\"\"\n",
    "    # Read in budget data using the function defined in the \n",
    "    # previous section\n",
    "    clean_budget_df = clean_budget()\n",
    "    # Everything except the dummy variable gets turned into\n",
    "    # real per-capita terms\n",
    "    scale_cols = clean_budget_df.columns.drop('heritage_dummy').tolist()\n",
    "    # Get population\n",
    "    per_capita = per_capita_data()\n",
    "    # Get inflation\n",
    "    inflation = inflation_data()\n",
    "    # Combine the datasets, can just use assign because they all\n",
    "    # have a datetime index\n",
    "    dfpc = (\n",
    "        clean_budget_df\n",
    "        .assign(pop=per_capita)\n",
    "        .assign(cpi=inflation)\n",
    "    )\n",
    "    # rescale to real per capita\n",
    "    dfpc[scale_cols] = (\n",
    "        dfpc[scale_cols]\n",
    "        # original data was in millions of dollars\n",
    "        .mul(1_000_000)\n",
    "        # divide by population and inflation for\n",
    "        # real per-capita\n",
    "        .div(dfpc['pop'], axis='index')\n",
    "        .div(dfpc['cpi'], axis='index')\n",
    "    )\n",
    "    return dfpc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exogenous factors\n",
    "\n",
    "The paper lists the Alberta employment rate, the Alberta unemployment rate, and the CAD/USD exchange rate as exogenous factors included in the model. Functions in this section acquire that data. I had to do some fiddling to get long enough historical series for some of the factors as you'll note in the code. It's hard to say for sure how the original author sourced this data. I'll just have to compare my tables and charts to his to see if I got close enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_historical_cad_usd() -> pd.DataFrame:\n",
    "    \"\"\"Get exchange rates from before 1971.\n",
    "\n",
    "    FRED live data only goes back to 1971, I need a longer series\n",
    "    This was what I could find. It's annual only, so I can't do it on a budget\n",
    "    year basis, but hopefully it will be close enough\n",
    "\n",
    "    This whole function is just some gross munging to read in a table from a web page.\n",
    "    Once it's called we save it to the data folder so I don't have to re-call it every\n",
    "    time I run this notebook.\n",
    "    \"\"\"\n",
    "    url = 'https://fxtop.com/en/historical-exchange-rates.php?YA=1&C1=USD&C2=CAD&A=1&YYYY1=1953&MM1=01&DD1=01&YYYY2=2019&MM2=04&DD2=01&LANG=en'\n",
    "    df = pd.read_html(url)[29]\n",
    "    headers = df.iloc[0]\n",
    "    new_df  = (\n",
    "        pd.DataFrame(df.values[1:], columns=headers)\n",
    "        .rename(columns={'Year': 'year', 'Average USD/CAD': 'EXCAUS'})\n",
    "        .assign(month=1)\n",
    "        .assign(day=1)\n",
    "        .assign(budget_dt=lambda df: pd.to_datetime(df[['year', 'month', 'day']]))\n",
    "        .set_index('budget_dt')\n",
    "        .reindex(columns=['EXCAUS'])\n",
    "    )\n",
    "    new_df.to_csv('./data/early_cad_usd.csv')\n",
    "    return new_df\n",
    "\n",
    "\n",
    "def read_historical_cad_usd(force_update: bool = False) -> pd.DataFrame:\n",
    "    \"\"\"Get exchange rates before 1971.\n",
    "\n",
    "    This wraps the above function to read in the downloaded data\n",
    "    if it's available and download and then read it if required.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    force_update: bool\n",
    "        Download the data set even if you already have it\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Exchange rates from 1965 to 1971\n",
    "    \"\"\"\n",
    "    fname = Path('.').joinpath('data').joinpath('early_cad_usd.csv')\n",
    "    if not fname.exists() or force_update:\n",
    "        return download_historical_cad_usd()\n",
    "    else:\n",
    "        return pd.read_csv(fname).set_index('budget_dt')\n",
    "\n",
    "\n",
    "def download_cad_usd() -> pd.DataFrame:\n",
    "    \"\"\"Download monthly exchange data from FRED.\n",
    "\n",
    "    For most of the period of interest I can get monthly\n",
    "    data from FRED, so I'll do that where possible.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Most of the CAD/USD exchange data I need for this analysis.\n",
    "    \"\"\"\n",
    "    df = pdr.get_data_fred('EXCAUS', start=dt.date(1970, 1, 1))\n",
    "    df.to_csv('./data/cad_usd.csv')\n",
    "    return df\n",
    "\n",
    "\n",
    "def read_cad_usd(force_update=False):\n",
    "    \"\"\"Get monthly exchange data from FRED.\n",
    "\n",
    "    This wraps the above function to read in the downloaded data\n",
    "    if it's available and download and then read it if required.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    force_update: bool\n",
    "        Download the data set even if you already have it\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Exchange rate data\n",
    "    \"\"\"\n",
    "    fname = Path('.').joinpath('data').joinpath('cad_usd.csv')\n",
    "    if not fname.exists() or force_update:\n",
    "        return download_cad_usd()\n",
    "    else:\n",
    "        return pd.read_csv(fname, parse_dates=['DATE']).set_index('DATE')\n",
    "\n",
    "\n",
    "def annual_cad_usd() -> pd.DataFrame:\n",
    "    \"\"\"Full series of CAD/USD in fiscal year format.\n",
    "\n",
    "    Get FRED data and turn the monthly values into annualized on a budget\n",
    "    basis for as much as possible. Fill in the remainder with calendar annual\n",
    "    data from fxtop\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Exchange data on an annualized basis.\n",
    "    \"\"\"\n",
    "    # Create a datetime index of all the points we need\n",
    "    annual_date_range = pd.date_range('1964-01-01', '2018-01-01', freq='AS', name='budget_dt')\n",
    "    # Get the old annual stuff to fill in later\n",
    "    old_df = read_historical_cad_usd()\n",
    "    df = (\n",
    "        # get the monthly series\n",
    "        read_cad_usd()\n",
    "        # annualize it\n",
    "        .pipe(periodic_to_budget_annual, 'DATE', 12)\n",
    "        # add in all the missing dates we need\n",
    "        .reindex(annual_date_range)\n",
    "        # fill those missing dates from the old annual data set.\n",
    "        .fillna(old_df)\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "def stats_can_exog() -> pd.DataFrame:\n",
    "    \"\"\"Bring in exogenous StatsCan data. Employment and Unemployment rates.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Exogenous data required from StatsCan\n",
    "    \"\"\"\n",
    "    # Vectors for monthly series where available\n",
    "    ur_vec = \"v2064516\"\n",
    "    er_vec = \"v2064518\"\n",
    "    annual_date_range = pd.date_range('1964-01-01', '2018-01-01', freq='AS', name='budget_dt')\n",
    "    # for the earlier periods we only have annual data\n",
    "    old_df = (\n",
    "        stats_can.table_to_df('36-10-0345-01', path='data')\n",
    "        # Get Alberta data only\n",
    "        .loc[lambda x: x['GEO'] == 'Alberta']\n",
    "        # Keep only the categories we care about\n",
    "        .loc[lambda x: x['Economic indicators'].isin(['Population', 'Total employment', 'Unemployment rate'])]\n",
    "        # pivot so the year is the row and the variables are the columns\n",
    "        .pivot_table(index='REF_DATE', columns='Economic indicators', values='VALUE')\n",
    "        .rename(columns={'Unemployment rate': 'unemployment_rate'})\n",
    "        # calculate the employment rate\n",
    "        .assign(employment_rate=lambda x: (x['Total employment'] / x['Population']) * 100)\n",
    "        # drop the population, just used for calculating employment rate\n",
    "        .reindex(columns=['unemployment_rate', 'employment_rate'])\n",
    "        .rename_axis('budget_dt', axis='index')\n",
    "        .rename_axis(None, axis='columns')\n",
    "    )\n",
    "    # Get monthly data where available\n",
    "    df = (\n",
    "        stats_can.vectors_to_df_local([ur_vec, er_vec], path='data', start_date=dt.date(1964, 1, 1))\n",
    "        .rename(columns={ur_vec: 'unemployment_rate', er_vec: 'employment_rate'})\n",
    "        # annualize\n",
    "        .pipe(periodic_to_budget_annual, 'REF_DATE', 12)\n",
    "        # get the full range of data we want\n",
    "        .reindex(annual_date_range)\n",
    "        # fill in the gaps with the old annual series\n",
    "        .fillna(old_df)\n",
    "        # Not ideal but even the annual series doesn't go quite back\n",
    "        # far enough so we have to backfill the earliest available\n",
    "        # data point\n",
    "        .fillna(method='bfill')\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "def exogenous_variables() -> pd.DataFrame:\n",
    "    \"\"\"Bring in exogenous parameters together.\n",
    "\n",
    "    From the paper:\n",
    "    We also include other exogenous variables that are likely to affect\n",
    "    the province’s budget. It is known that the various components of the\n",
    "    provincial budget can be influenced by the business cycle. Thus, following\n",
    "    Buettner and Wildsain (2006), we account for the potential effects of the\n",
    "    business cycle by including one-period lagged changes in the provincial\n",
    "    employment and unemployment rates. Another important exogenous factor\n",
    "    that is often cited in provincial budget documents as being important in\n",
    "    influencing the provincial government’s oil royalty revenue is the Canadian-U.S.\n",
    "    dollar exchange rate. For this reason, we control for this factor by\n",
    "    including one period lagged changes in the Canadian-U.S. dollar exchange rate\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        All the necessary exogenous factors for reproducing the paper.\n",
    "    \"\"\"\n",
    "    cadusd = annual_cad_usd()\n",
    "    ur_er = stats_can_exog()\n",
    "    df = pd.concat([cadusd, ur_er], axis='columns')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Figures\n",
    "\n",
    "### Figure 1\n",
    "Page 5 of the report charts Non-renewable Resource Revenue, Total Expenditure, and Total Revenue. All are in per-capita 2017 dollars.\n",
    "Reproducing this chart will be a good starting check that my data extraction and transformation matches the original author's strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = budget_real_per_capita()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart_df = (\n",
    "    df\n",
    "    .loc['1970':'2016', ['natural_resource_revenue', 'total_revenue', 'total_expenditure', 'deficit']]\n",
    "    .rename(columns={\n",
    "        'natural_resource_revenue': 'Non-renewable Resource Revenue',\n",
    "        'total_revenue': 'Total Revenue',\n",
    "        'total_expenditure': 'Total Expenditure'\n",
    "    })\n",
    "    .reset_index()\n",
    "    .melt(id_vars='budget_dt')\n",
    ")\n",
    "chart = (\n",
    "    alt.Chart(chart_df)\n",
    "    .mark_line()\n",
    "    .encode(\n",
    "        x=alt.X('budget_dt:T', axis=alt.Axis(title=None)),\n",
    "        y=alt.Y('value:Q', axis=alt.Axis(title='Per capita in 2017 dollars')),\n",
    "        color=alt.Color('variable:N', legend=alt.Legend(title=None, orient='left'))\n",
    "    )\n",
    "    .properties(width=800, height=400)\n",
    ")\n",
    "chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.renderers.enable(\"jupyterlab\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This graph looks very similar to the chart in the paper, with a notable exception of the 1976/1977 budget year. My chart shows Non-renewable Resource Revenue as slightly negative, whereas the original chart has it largely in line with 1975/1976 and 1977/1978. NRR is negative in my chart because I have netted out contributions to the Alberta Heritage Savings Trust Fund (AHSTF). To the best of my understanding, the original paper does the same, and the consistent values between the two in all other years supports that. Quoting the original paper:\n",
    "\n",
    ">The part of resource revenue that is saved in the AHSTF is not expected to influence the provincial government’s spending and revenue-raising choices. For this reason, in our analysis, we exclude the part of the resource revenue that is saved in the AHSTF from the non-renewable-resource revenue data. \n",
    "\n",
    "For comparison, here is the same chart, but without netting AHSTF contributions from resource revenue, it's still netted out of total revenue, but this is just for comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart_df = (\n",
    "    df\n",
    "    .loc['1970':'2016', ['natural_resource_revenue_before_heritage', 'total_revenue', 'total_expenditure']]\n",
    "    .rename(columns={\n",
    "        'natural_resource_revenue_before_heritage': 'Non-renewable Resource Revenue (before AHSTF contributions)',\n",
    "        'total_revenue': 'Total Revenue',\n",
    "        'total_expenditure': 'Total Expenditure'\n",
    "    })\n",
    "    .reset_index()\n",
    "    .melt(id_vars='budget_dt')\n",
    ")\n",
    "chart = (\n",
    "    alt.Chart(chart_df)\n",
    "    .mark_line()\n",
    "    .encode(\n",
    "        x=alt.X('budget_dt:T', axis=alt.Axis(title=None)),\n",
    "        y=alt.Y('value:Q', axis=alt.Axis(title='Per capita in 2017 dollars')),\n",
    "        color=alt.Color('variable:N', legend=alt.Legend(title=None, orient='left'))\n",
    "    )\n",
    "    .properties(width=800, height=400)\n",
    ")\n",
    "chart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1976/1977 more closely matches the original chart in the paper, but the remaining years in the period of mid 70s to mid 80s when there were significant contributions clearly do not match. Going forward all analysis in this reproduction will treat NRR as net of AHSTF contributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 2\n",
    "\n",
    "Page 6 of the paper produces a scatter plot of Real per capita non-renewable resource revenue on the X axis vs. Real per capita budget balance on the Y, along with a linear trend fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize':(11.7,8.27)})\n",
    "chart_df = (\n",
    "    df\n",
    "    .loc['1970':'2016', ['natural_resource_revenue', 'deficit']]\n",
    "    .rename(columns={\n",
    "        'natural_resource_revenue': 'Non-renewable Resource Revenue',\n",
    "        'deficit': 'Deficit'\n",
    "    })\n",
    "    .assign(balance=lambda df: df['Deficit'] * -1)\n",
    "    .rename(columns={'balance': 'Budget Balance'})\n",
    "    .copy()\n",
    ")\n",
    "sns.regplot(x='Non-renewable Resource Revenue', y='Budget Balance', data=chart_df);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the exception of the outlier previously noted in the time series view of the plot, this representation also looks very similar to what was in the original paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Specification and estimation\n",
    "\n",
    "This section combines the previously specified data extraction with transformations necessary to produce summary statistics, statistical tests, and the VAR model itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_df_levels():\n",
    "    \"\"\"Combine real per capita budget data to get model data in levels\n",
    "    \n",
    "    lag exogenous variables (unemployment and employment rates, CAD/USD exchange)\n",
    "    \"\"\"\n",
    "    budg = budget_real_per_capita()\n",
    "    exog = exogenous_variables()\n",
    "    df = (\n",
    "        pd.concat([budg, exog], axis='columns')\n",
    "        .rename(columns={'total_prog_exp': 'program_expenditure', 'EXCAUS': 'cad_usd'})\n",
    "        .assign(ur_lag=lambda df: df['unemployment_rate'].shift(periods=1))\n",
    "        .assign(er_lag=lambda df: df['employment_rate'].shift(periods=1))\n",
    "        .assign(cad_usd_lag=lambda df: df['cad_usd'].shift(periods=1))\n",
    "        .reindex(columns=[\n",
    "            'program_expenditure', 'debt_service', 'corporate_income_tax',\n",
    "            'personal_income_tax', 'other_revenue', 'natural_resource_revenue',\n",
    "            'deficit', 'heritage_dummy', 'ur_lag', 'er_lag', 'cad_usd_lag'\n",
    "        ])\n",
    "    )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdfl = model_df_levels()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sumary statistics for key variables, 1970-71, 2016-17 in levels\n",
    "\n",
    "Reproduce the top half of table 1 from the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number = \"{:0<4,.1f}\"\n",
    "percent = '{:.1%}'\n",
    "count = \"{:0.0f}\"\n",
    "\n",
    "df = (\n",
    "    mdfl\n",
    "    .loc['1970':'2016']\n",
    "    .copy()\n",
    "    .drop(columns=['heritage_dummy'])\n",
    "    .reindex(columns=[\n",
    "        'natural_resource_revenue', 'corporate_income_tax', 'personal_income_tax',\n",
    "        'other_revenue', 'debt_service', 'program_expenditure', 'deficit', 'ur_lag',\n",
    "        'er_lag', 'cad_usd_lag'\n",
    "    ])\n",
    "    .describe()\n",
    "    .T\n",
    "    .style.format({\n",
    "        'count': count,\n",
    "        'mean': number,\n",
    "        'std': number,\n",
    "        'min': number,\n",
    "        '25%': number,\n",
    "        '50%': number,\n",
    "        '75%': number,\n",
    "        'max': number\n",
    "    })\n",
    ")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the figures that I can validate against (exogenous variables aren't reported in the paper) are reasonably close. The one noted difference is the previously described outlier in natural resource revenue which leads to my minimum for that variable being significantly lower than in the paper. My guess for the observed discrepancies are differences in calculating population or CPI. It will be interesting to see how sensitive the rest of the model is to these relatively small differences in transformation methodology."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sumary statistics for key variables, 1970-71, 2016-17, first difference\n",
    "\n",
    "Reproduce the bottom half of table 1 from the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_df_first_diff(mdfl):\n",
    "    \"\"\"Produce the first difference of the level model df\"\"\"\n",
    "    df = (\n",
    "        mdfl\n",
    "        .diff()\n",
    "        .loc['1970':'2016']\n",
    "        .copy()\n",
    "        .assign(heritage_dummy=mdfl['heritage_dummy']) # don't want to lag diff this\n",
    "        .assign(constant=1)\n",
    "        .assign(zero=0)\n",
    "        .assign(nrrd=lambda df: df[['natural_resource_revenue', 'zero']].min(axis='columns'))\n",
    "        .assign(nrri=lambda df: df[['natural_resource_revenue', 'zero']].max(axis='columns'))\n",
    "        .reindex(columns=[\n",
    "            'natural_resource_revenue', 'nrri', 'nrrd', 'corporate_income_tax', 'personal_income_tax',\n",
    "            'other_revenue', 'debt_service', 'program_expenditure', 'deficit', 'ur_lag',\n",
    "            'er_lag', 'cad_usd_lag', 'heritage_dummy', 'constant'\n",
    "        ])\n",
    "    )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (\n",
    "    model_df_first_diff(mdfl)\n",
    "    .drop(columns=['heritage_dummy', 'constant'])\n",
    "    .describe()\n",
    "    .T\n",
    "    .style.format({\n",
    "        'count': count,\n",
    "        'mean': number,\n",
    "        'std': number,\n",
    "        'min': number,\n",
    "        '25%': number,\n",
    "        '50%': number,\n",
    "        '75%': number,\n",
    "        'max': number\n",
    "    })\n",
    ")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The outlier in natural resource revenue really skews the first difference max and min, and increases the standard deviation. Again, it will be interesting to see how this impacts the parameter estimates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw tables\n",
    "For the purposes of validation, here are the full tables I used to produce both the summary statistics above, as well as all statistical models and tests below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdfl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_df_first_diff(mdfl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unit-Root Tests\n",
    "\n",
    "Table A1 in the paper shows the results of unit root tests for both the level and first differenced variables in the model. This section will reproduce those tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "level_stationarity_df = (\n",
    "    mdfl\n",
    "    .loc['1970':'2016']\n",
    "    .copy()\n",
    "    .drop(columns=['heritage_dummy'])\n",
    "    .reindex(columns=[\n",
    "        'natural_resource_revenue', 'corporate_income_tax', 'personal_income_tax',\n",
    "        'other_revenue', 'debt_service', 'program_expenditure', 'deficit', 'ur_lag',\n",
    "        'er_lag', 'cad_usd_lag'\n",
    "    ])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_diff_stationarity_df = (\n",
    "    model_df_first_diff(mdfl)\n",
    "    .drop(columns=['heritage_dummy', 'constant'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stationarity_tests(df):\n",
    "    tests_dict = {'ADF': ADF, 'Phillips-Perron': PhillipsPerron, 'DF-GLS': DFGLS} \n",
    "    cols = df.columns\n",
    "    tests_df = pd.DataFrame()\n",
    "    for test_label, test in tests_dict.items():\n",
    "        for col in cols:\n",
    "            if test_label != 'Phillips-Perron':\n",
    "                col_test = test(df[col].dropna(), method='BIC')\n",
    "            else:\n",
    "                col_test = test(df[col].dropna())\n",
    "            test_val = col_test.stat\n",
    "            test_p = col_test.pvalue\n",
    "            test_summary = f'val: {test_val:0.3f}, p: {test_p:.1%}'\n",
    "            tests_df.loc[col, test_label] = test_summary\n",
    "    return tests_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stationarity_tests(level_stationarity_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stationarity_tests(first_diff_stationarity_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Documentation on the test tools I used can be found [here](https://arch.readthedocs.io/en/latest/unitroot/tests.html)\n",
    "\n",
    "There are some interesting differences. Most notable is that on the levels of the deficit series I reject the null hypothesis of a unit root using all three tests at a significance level < 1%. The paper specifically notes that if the deficit is stationary in levels then a Vector Error Correction model can be applied. As the original author's fails to reject the null he implements a Vector AutoRegression model on the first differenced data. In levels the only other series that I find to be stationary is natural resource revenue. ADF on program expenditure would also reject the null at 5% significance, but would fail to reject it using the other two tests.\n",
    "\n",
    "Looking at the first differenced series, since that's what the paper ultimately ends up using, I also reject the null hypothesis of a unit root for all variables using all tests at a 1% significant *except* program expenditure and deficit using Augmented Dickey Fuller. Those last two tests differ from what's reported in the paper. \n",
    "\n",
    "The paper notes that it uses the Schwarz Information Criterion (SIC) for determining optimal lags in the DF-GLS test. It doesn't specify what it's using in the other two tests. For ADF and DF-GLS I used the Schwarz/Bayesian IC (BIC), [which is just another name for SIC](https://en.wikipedia.org/wiki/Bayesian_information_criterion). Phillips-Perron only uses 1 lag and then Newey-West for a long run variance estimator. I also ran these tests using Akaike IC (AIC) for optimal lags for ADF and DF-GLS, with similar results. I'm not sure what explains the discrepancies in my results with what's reported in the paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Model\n",
    "\n",
    "Take the first differenced dataframe and produce a VAR model using it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_df = (\n",
    "    model_df_first_diff(mdfl)\n",
    "    .drop(columns='natural_resource_revenue')\n",
    "    .dropna()\n",
    ")\n",
    "endog_df = vec_df[[\n",
    "    'nrri', 'nrrd', 'program_expenditure', 'debt_service', 'corporate_income_tax',\n",
    "     'personal_income_tax', 'other_revenue'\n",
    "]]\n",
    "exog_df = vec_df[['ur_lag', 'er_lag', 'cad_usd_lag', 'heritage_dummy']]\n",
    "model = VAR(endog=endog_df, exog=exog_df, freq='AS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model with 2 lags\n",
    "results = model.fit(2)\n",
    "summary = results.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't show exogenous parameters since I have nothing to compare them to\n",
    "results.params.loc['L1.nrri':]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def highlight_significance(val):\n",
    "    \"\"\"\n",
    "    Takes a scalar and returns a string with\n",
    "    the css property `'color: <color>'` where\n",
    "    color is maroon for 1% significance, \n",
    "    red for 5% significance,\n",
    "    orange for 10%, and black otherwise\n",
    "    \"\"\"\n",
    "    \n",
    "    if val <= 0.01:\n",
    "        color = 'maroon'\n",
    "    elif val <= 0.05:\n",
    "        color = 'red'\n",
    "    elif val <= 0.1:\n",
    "        color = 'orange'\n",
    "    else:\n",
    "        color = 'black'\n",
    "    return f'color: {color}'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.pvalues.loc['L1.nrri':].style.applymap(highlight_significance).format(\"{:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well this is weird, my coefficient estimates are significantly different than what's in the original paper. Notably I show an increase in program spending in response to an increase *or* a decrease in resource revenue in the first lag, although neither are statistically significant. This is a pretty stark contrast to the original paper, which has the more intuitive result of program spending increasing after a positive resource revenue shock and decreasing with a negative shock (although only the positive response is statistically significant).\n",
    "\n",
    "At this point I can't account for the discrepancy. Some of my earlier results differed, due to the noted difference in resource revenue in the one year, as well as minor discrepancies presumably based on me doing slightly different real per capita calculations, but I'm surprised the change is this big.\n",
    "\n",
    "I'll see what my coefficients imply for the rest of the results, but I imagine they're going to be pretty different going forward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impulse Response Functions\n",
    "\n",
    "The actual results of the paper involve taking the estimated impulse response functions derived from the VAR model and examining their implications. Let's try and reproduce that now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "irf = results.irf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Table 3\n",
    "IMPACTS ON ALBERTA’S BUDGET OF A ONE-DOLLAR INNOVATION IN NON-RENEWABLE-RESOURCE\n",
    "REVENUE (ASYMMETRIC CASE), 1970/71–2016/17 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "irfs = irf.irfs\n",
    "irf_stderr = irf.stderr()\n",
    "params = list(results.params.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impulse_response(impulse, response):\n",
    "    imp_ind = params.index(impulse)\n",
    "    res_ind = params.index(response)\n",
    "    ir = irfs[:, res_ind, imp_ind]\n",
    "    se = irf_stderr[:, res_ind, imp_ind]\n",
    "    imp_name = response + '_impulse'\n",
    "    se_name = response + '_se'\n",
    "    df = pd.DataFrame({imp_name: ir, se_name: se})\n",
    "    return df.loc[1:3].T\n",
    "\n",
    "def ir_table(impulse):\n",
    "    responses = params[2:]\n",
    "    return pd.concat([impulse_response(impulse, response) for response in responses])\n",
    "ir_table('nrrd')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Figure 3a\n",
    "The total response of program spending to a one-dollar increase in NRR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/statsmodels/statsmodels/issues/1265\n",
    "_ = irf.plot(impulse='nrri', response='program_expenditure')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Figure 3b\n",
    "The total response of program spending to a one-dollar decrease in NRR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = irf.plot(impulse='nrrd', response='program_expenditure')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "Figure 4a, 4b, 5, 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
